(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{407:function(t,a,s){t.exports=s.p+"assets/img/chat_ui.4ac32c7c.png"},433:function(t,a,s){"use strict";s.r(a);var n=s(1),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,n=t._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"title"}),n("p",[t._v("This post is about building a local knowledge base using Large Language Models (LLMs) and Langchain.")])]),t._v(" "),n("h2",{attrs:{id:"intro"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#intro"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),n("p",[t._v("Large Language Models (LLMs) are a groundbreaking advancement in the field of natural language processing and artificial intelligence. These models have the ability to understand and generate human-like text, making them incredibly powerful tools for a wide range of applications. LLMs are trained on vast amounts of data, allowing them to learn the intricacies of language and context. They can be used for tasks such as text generation, translation, summarization, and even conversation. With their ability to comprehend and generate coherent and contextually relevant text, LLMs have the potential to revolutionize various industries, including content creation, customer service, and research. As these models continue to evolve and improve, they hold the promise of transforming the way we interact with and utilize language in the digital world.")]),t._v(" "),n("p",[t._v("Using Large Language Models (LLMs) to build local knowledge bases is a groundbreaking approach that combines the power of natural language processing with the ability to store and retrieve information specific to a particular domain or locality. LLMs, with their remarkable language understanding capabilities, can be leveraged to create intelligent systems that not only comprehend and generate human-like text but also possess a vast amount of contextual knowledge. By training LLMs on local data sources such as regional news articles, historical records, or community forums, we can create knowledge bases that capture the unique characteristics and nuances of a specific location or community. These local knowledge bases can then be utilized to provide accurate and relevant information, answer queries, and assist users in a variety of applications, including virtual assistants, chatbots, and recommendation systems. By harnessing the power of LLMs, we can build intelligent systems that not only understand language but also possess a deep understanding of the local context, enabling more personalized and contextually relevant interactions with users.")]),t._v(" "),n("p",[n("a",{attrs:{href:"https://python.langchain.com",target:"_blank",rel:"noopener noreferrer"}},[t._v("LangChain"),n("OutboundLink")],1),t._v(" is a framework for developing applications powered by language models. It enables applications that are:")]),t._v(" "),n("ul",[n("li",[t._v("Data-aware: connect a language model to other sources of data")]),t._v(" "),n("li",[t._v("Agentic: allow a language model to interact with its environment")])]),t._v(" "),n("p",[t._v("The main value props of LangChain are:")]),t._v(" "),n("ul",[n("li",[t._v("Components: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not")]),t._v(" "),n("li",[t._v("Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks")])]),t._v(" "),n("p",[t._v("Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.")]),t._v(" "),n("p",[t._v("Combine the power of the LLMs and the Langchain library, it is now possible to build a local knowledge base application in just a few hundreds lines of code.")]),t._v(" "),n("h2",{attrs:{id:"overall-design-of-the-application"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#overall-design-of-the-application"}},[t._v("#")]),t._v(" Overall Design of the application")]),t._v(" "),n("p",[t._v("For this application, we are going to build the following components:")]),t._v(" "),n("ul",[n("li",[t._v("A document loader that will load text chunks from different file types such as pdf, epub, docx, txt, etc.")]),t._v(" "),n("li",[t._v("An "),n("a",{attrs:{href:"https://platform.openai.com/docs/guides/embeddings",target:"_blank",rel:"noopener noreferrer"}},[t._v("embedding"),n("OutboundLink")],1),t._v(" engine to convert text chunks into embeddings.")]),t._v(" "),n("li",[t._v("A vector database to save embeddings")]),t._v(" "),n("li",[t._v("Finally, let the LLMs to query the vector database to find the relevant answers to user queries.")])]),t._v(" "),n("h2",{attrs:{id:"text-loader"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#text-loader"}},[t._v("#")]),t._v(" Text Loader")]),t._v(" "),n("p",[n("code",[t._v("langchain")]),t._v(" has text loaders for a variety of document format. It also has different text splitters to convert the document into chunks of "),n("a",{attrs:{href:"https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them",target:"_blank",rel:"noopener noreferrer"}},[t._v("tokens"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("document_loaders "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    PyPDFLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    TextLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    Docx2txtLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    DirectoryLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    UnstructuredEPubLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("docstore"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("document "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Document\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text_splitter "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" RecursiveCharacterTextSplitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TextSplitter\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_document")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc_type"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pdf"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""parse document files and chunking into list of Document"""')]),t._v("\n    parse_funcs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pdf"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" parse_pdf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"docx"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" parse_docx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"txt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" parse_txt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"epub"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" parse_epub"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    default_text_splitter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RecursiveCharacterTextSplitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        chunk_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        chunk_overlap"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        length_function"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        add_start_index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" doc_type "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" parse_funcs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parse_funcs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("doc_type"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" parse_funcs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("doc_type"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default_text_splitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" NotImplementedError"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Document type ')]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("doc_type"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v(' is not supported!"')])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_epub")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text_splitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" TextSplitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Document"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""parse epub document"""')]),t._v("\n    loader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" UnstructuredEPubLoader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pages "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_and_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text_splitter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" pages\n")])])]),n("h2",{attrs:{id:"embeddings-and-vector-databases"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#embeddings-and-vector-databases"}},[t._v("#")]),t._v(" Embeddings and Vector databases")]),t._v(" "),n("p",[t._v("For the embedding engine, we can use OpenAI's "),n("a",{attrs:{href:"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings",target:"_blank",rel:"noopener noreferrer"}},[t._v("embedding API"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("p",[t._v("For the vector database, there are many choices such as "),n("a",{attrs:{href:"https://github.com/facebookresearch/faiss",target:"_blank",rel:"noopener noreferrer"}},[t._v("FAISS"),n("OutboundLink")],1),t._v(", "),n("a",{attrs:{href:"https://www.pinecone.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pinecone"),n("OutboundLink")],1),t._v(" and "),n("a",{attrs:{href:"https://www.trychroma.com/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Chroma"),n("OutboundLink")],1),t._v(". Here we use "),n("code",[t._v("Chroma")]),t._v(" as the vector database.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KnowledgeBase")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("init_chromadb")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Document"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        client_settings "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chromadb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configure"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            chroma_db_impl"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"duckdb+parquet"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            persist_directory"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"db"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            anonymized_telemetry"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        embeddings "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" OpenAIEmbeddings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("client"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        vectorstore "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Chroma"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            collection_name"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test01"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            embedding_function"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("embeddings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            client_settings"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("client_settings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            persist_directory"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"db"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        vectorstore"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embedding"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("embeddings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        vectorstore"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("persist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" vectorstore\n\n    "),n("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("query_chromadb")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        client_settings "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chromadb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configure"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            chroma_db_impl"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"duckdb+parquet"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            persist_directory"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"db"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            anonymized_telemetry"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        embeddings "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" OpenAIEmbeddings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("client"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        vectorstore "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Chroma"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            collection_name"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test01"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            embedding_function"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("embeddings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            client_settings"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("client_settings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            persist_directory"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"db"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" vectorstore"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("similarity_search"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"query-the-vector-database"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#query-the-vector-database"}},[t._v("#")]),t._v(" Query the Vector Database")]),t._v(" "),n("p",[n("code",[t._v("langchain")]),t._v(" provides different "),n("a",{attrs:{href:"https://python.langchain.com/docs/modules/chains/",target:"_blank",rel:"noopener noreferrer"}},[t._v("chains"),n("OutboundLink")],1),t._v(" to process different types of user queries through LLMs.")]),t._v(" "),n("p",[t._v("For the LLM, we also used OpenAI's gpt-3.5-turbo model, which is also the default OpenAI chat model in "),n("code",[t._v("langchain")]),t._v(".")]),t._v(" "),n("p",[t._v("For the "),n("code",[t._v("langchain")]),t._v(" chain object, we use the "),n("code",[t._v("load_qa_with_sources_chain")]),t._v(' for the "ask and answer" type of user queries.')]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KnowledgeBase")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("async")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("ask")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""query the knowledge base"""')]),t._v("\n        relavent_docs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("query_chromadb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        llm "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatOpenAI"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            temperature"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            client"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# streaming=True,")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# callbacks=[StreamingStdOutCallbackHandler()],")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        prompt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt_template"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        chain "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_qa_with_sources_chain"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            llm"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("llm"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" chain_type"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"stuff"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prompt"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("prompt\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        result "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chain"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arun"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            input_documents"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("relavent_docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            question"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            return_only_outputs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" result\n")])])]),n("h2",{attrs:{id:"final-touch-chat-ui-with-nicegui"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#final-touch-chat-ui-with-nicegui"}},[t._v("#")]),t._v(" Final touch - Chat UI with NiceGUI")]),t._v(" "),n("p",[t._v("Lastly, we can use a web UI framework to quickly add a user interface for the local knowledge base.")]),t._v(" "),n("p",[t._v('As we can see in the screenshot below, the local knowledge base will be able to answer user queries using the information in the uploaded documents with a reference to the source document. If a user asks a non-relevant question, it will just answer "I don\'t know" instead of a hallucination answer like the generic ChatGPT would do.')]),t._v(" "),n("p",[n("img",{attrs:{src:s(407),alt:"KnowledgeBase UI"}})])])}),[],!1,null,null,null);a.default=e.exports}}]);